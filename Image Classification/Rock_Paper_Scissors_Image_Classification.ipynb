{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rock Paper Scissors Image Classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx9MUB2d-KS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c312fb-b6bd-4ea6-bc68-6cddb2106678"
      },
      "source": [
        "#get Dataset\n",
        "!wget --no-check-certificate \\\n",
        "  https://dicodingacademy.blob.core.windows.net/picodiploma/ml_pemula_academy/rockpaperscissors.zip \\\n",
        "  -O /tmp/rockpaperscissors.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-30 10:59:57--  https://dicodingacademy.blob.core.windows.net/picodiploma/ml_pemula_academy/rockpaperscissors.zip\n",
            "Resolving dicodingacademy.blob.core.windows.net (dicodingacademy.blob.core.windows.net)... 52.239.197.36\n",
            "Connecting to dicodingacademy.blob.core.windows.net (dicodingacademy.blob.core.windows.net)|52.239.197.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322873683 (308M) [application/zip]\n",
            "Saving to: ‘/tmp/rockpaperscissors.zip’\n",
            "\n",
            "/tmp/rockpapersciss 100%[===================>] 307.92M  3.09MB/s    in 71s     \n",
            "\n",
            "2020-11-30 11:01:09 (4.35 MB/s) - ‘/tmp/rockpaperscissors.zip’ saved [322873683/322873683]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okLHK5XBsvYd"
      },
      "source": [
        "#Import Lib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import zipfile,os\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcerQxPntQ2Z"
      },
      "source": [
        "#Create Directory\n",
        "zip_dir = '/tmp/rockpaperscissors.zip'\n",
        "extract = zipfile.ZipFile(zip_dir, 'r')\n",
        "extract.extractall('/tmp')\n",
        "extract.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_rCLRnr_l5i"
      },
      "source": [
        "data_dir = '/tmp/rockpaperscissors/rps-cv-images'\n",
        "rock_dir = os.path.join(data_dir, 'rock')\n",
        "paper_dir = os.path.join(data_dir, 'paper')\n",
        "scissors_dir = os.path.join(data_dir, 'scissors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lMyx8RDAzYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4000f65a-7040-4e3d-b580-01f4c183c2bd"
      },
      "source": [
        "os.listdir(data_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README_rpc-cv-images.txt', 'scissors', 'rock', 'paper']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrQKE0Q_A4lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458a42cc-dbe8-4ed1-f706-9b77df0a6912"
      },
      "source": [
        "#Length Dataset\n",
        "print('Data Paper :',len(os.listdir(paper_dir)))\n",
        "print('Data Rock :',len(os.listdir(rock_dir)))\n",
        "print('Data Scissors :',len(os.listdir(scissors_dir)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Paper : 712\n",
            "Data Rock : 726\n",
            "Data Scissors : 750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L547HsQFu7gN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de00464c-6b1f-4430-cd19-38b94efe3e6f"
      },
      "source": [
        "data1 = len(os.listdir(paper_dir))\n",
        "data2 = len(os.listdir(rock_dir))\n",
        "data3 = len(os.listdir(scissors_dir))\n",
        "jumlah = data1 + data2 + data3\n",
        "print('Total Sampel Gambar :',jumlah) #Sum total dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sampel Gambar : 2188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTeMyu5alwgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb282213-3bd8-454e-e7fd-4b7200081ec0"
      },
      "source": [
        "dirr = '/tmp/rockpaperscissors/rps-cv-images'\n",
        "augmen = ImageDataGenerator( #Image Augmentation\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip = True,\n",
        "    height_shift_range = .2,\n",
        "    vertical_flip = True,\n",
        "    validation_split = 0.4\n",
        ")\n",
        "trg_gen = augmen.flow_from_directory( #Train generator\n",
        "    dirr,\n",
        "    target_size = (220,220),\n",
        "    batch_size = 5,\n",
        "    class_mode = 'categorical',\n",
        "    shuffle = True,\n",
        "    classes = ['rock','paper','scissors'],\n",
        "    subset = 'training'\n",
        ")\n",
        "val_gen = augmen.flow_from_directory( #Validate Generator\n",
        "    dirr,\n",
        "    target_size = (220,220),\n",
        "    batch_size = 5,\n",
        "    class_mode = 'categorical',\n",
        "    shuffle = False,\n",
        "    classes = ['rock','paper','scissors'],\n",
        "    subset = 'validation'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1314 images belonging to 3 classes.\n",
            "Found 874 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gJe6m__uLR3"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import MaxPooling2D,Conv2D,Dense,Flatten,Dropout\n",
        " #Create Model\n",
        "model = Sequential([\n",
        "                    Conv2D(32, (3,3), activation='relu', input_shape=(220, 220, 3)),\n",
        "                    MaxPooling2D(2,2),\n",
        "                    Conv2D(64, (3,3), activation='relu'),\n",
        "                    MaxPooling2D(2,2),\n",
        "                    Conv2D(128, (3,3), activation='relu'),\n",
        "                    MaxPooling2D(2,2),\n",
        "                    Conv2D(128, (3,3), activation='relu'),\n",
        "                    MaxPooling2D(2,2),\n",
        "                    Dropout(0.3),\n",
        "                    Flatten(),\n",
        "                    Dense(512, activation='relu'),\n",
        "                    Dense(3,activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67dOBQv8hzX8"
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "model.compile(loss='categorical_crossentropy', #Compiling Model\n",
        "              optimizer = RMSprop(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfXCQ99UZoOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da1e27e-5d8b-4790-f962-87a1600e9b4b"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler\n",
        " #Create Callbacks Func\n",
        "best_model_weights = '/tmp/base.model'\n",
        "checkpoint = ModelCheckpoint(\n",
        "    best_model_weights,\n",
        "    monitor='val_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    save_weights_only=False,\n",
        "    period=1\n",
        ")\n",
        "earlystop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    mode='auto'\n",
        ")\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir = '/tmp/logs',\n",
        "    histogram_freq=0,\n",
        "    batch_size=5,\n",
        "    write_graph=True,\n",
        "    write_grads=True,\n",
        "    write_images=False,\n",
        ")\n",
        "\n",
        "csvlogger = CSVLogger(\n",
        "    filename= \"training_csv.log\",\n",
        "    separator = \",\",\n",
        "    append = False\n",
        ")\n",
        "\n",
        "\n",
        "reduce = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=40,\n",
        "    verbose=1, \n",
        "    mode='auto',\n",
        "    cooldown=1 \n",
        ")\n",
        "\n",
        "callbacks = [checkpoint,tensorboard,csvlogger,reduce]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b219qC8li_Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c8d9c8-6c3a-4f30-befb-776c8fcc56b6"
      },
      "source": [
        "#Train Model\n",
        "model.fit(\n",
        "    trg_gen, \n",
        "    steps_per_epoch  = 50, \n",
        "    validation_data  = val_gen,\n",
        "    validation_steps = 5,\n",
        "    epochs = 20, \n",
        "    verbose = 1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " 1/50 [..............................] - ETA: 0s - loss: 1.1063 - accuracy: 0.4000WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.3164 - accuracy: 0.3320\n",
            "Epoch 00001: val_loss improved from inf to 1.07947, saving model to /tmp/base.model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /tmp/base.model/assets\n",
            "50/50 [==============================] - 33s 670ms/step - loss: 1.3164 - accuracy: 0.3320 - val_loss: 1.0795 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.1008 - accuracy: 0.4320\n",
            "Epoch 00002: val_loss improved from 1.07947 to 1.01813, saving model to /tmp/base.model\n",
            "INFO:tensorflow:Assets written to: /tmp/base.model/assets\n",
            "50/50 [==============================] - 33s 665ms/step - loss: 1.1008 - accuracy: 0.4320 - val_loss: 1.0181 - val_accuracy: 0.3200\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9939 - accuracy: 0.5560\n",
            "Epoch 00003: val_loss improved from 1.01813 to 0.32727, saving model to /tmp/base.model\n",
            "INFO:tensorflow:Assets written to: /tmp/base.model/assets\n",
            "50/50 [==============================] - 33s 661ms/step - loss: 0.9939 - accuracy: 0.5560 - val_loss: 0.3273 - val_accuracy: 0.9600\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8320 - accuracy: 0.6760\n",
            "Epoch 00004: val_loss did not improve from 0.32727\n",
            "50/50 [==============================] - 32s 639ms/step - loss: 0.8320 - accuracy: 0.6760 - val_loss: 0.6464 - val_accuracy: 0.8000\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5970 - accuracy: 0.7600\n",
            "Epoch 00005: val_loss did not improve from 0.32727\n",
            "50/50 [==============================] - 32s 636ms/step - loss: 0.5970 - accuracy: 0.7600 - val_loss: 0.3331 - val_accuracy: 0.9200\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6128 - accuracy: 0.7640\n",
            "Epoch 00006: val_loss improved from 0.32727 to 0.27463, saving model to /tmp/base.model\n",
            "INFO:tensorflow:Assets written to: /tmp/base.model/assets\n",
            "50/50 [==============================] - 33s 657ms/step - loss: 0.6128 - accuracy: 0.7640 - val_loss: 0.2746 - val_accuracy: 0.9200\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.8514\n",
            "Epoch 00007: val_loss did not improve from 0.27463\n",
            "50/50 [==============================] - 32s 632ms/step - loss: 0.5032 - accuracy: 0.8514 - val_loss: 0.4644 - val_accuracy: 0.8000\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4049 - accuracy: 0.8400\n",
            "Epoch 00008: val_loss did not improve from 0.27463\n",
            "50/50 [==============================] - 32s 633ms/step - loss: 0.4049 - accuracy: 0.8400 - val_loss: 0.3530 - val_accuracy: 0.9200\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 0.8560\n",
            "Epoch 00009: val_loss did not improve from 0.27463\n",
            "50/50 [==============================] - 32s 634ms/step - loss: 0.4989 - accuracy: 0.8560 - val_loss: 0.3673 - val_accuracy: 0.9600\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.8715\n",
            "Epoch 00010: val_loss improved from 0.27463 to 0.21763, saving model to /tmp/base.model\n",
            "INFO:tensorflow:Assets written to: /tmp/base.model/assets\n",
            "50/50 [==============================] - 32s 649ms/step - loss: 0.3503 - accuracy: 0.8715 - val_loss: 0.2176 - val_accuracy: 0.9600\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.9040\n",
            "Epoch 00011: val_loss did not improve from 0.21763\n",
            "50/50 [==============================] - 32s 633ms/step - loss: 0.5288 - accuracy: 0.9040 - val_loss: 0.6885 - val_accuracy: 0.7200\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.8720\n",
            "Epoch 00012: val_loss did not improve from 0.21763\n",
            "50/50 [==============================] - 32s 632ms/step - loss: 0.3471 - accuracy: 0.8720 - val_loss: 0.5093 - val_accuracy: 0.8800\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.8956\n",
            "Epoch 00013: val_loss did not improve from 0.21763\n",
            "50/50 [==============================] - 31s 629ms/step - loss: 0.2802 - accuracy: 0.8956 - val_loss: 0.3041 - val_accuracy: 0.9600\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.8835\n",
            "Epoch 00014: val_loss improved from 0.21763 to 0.17242, saving model to /tmp/base.model\n",
            "INFO:tensorflow:Assets written to: /tmp/base.model/assets\n",
            "50/50 [==============================] - 33s 656ms/step - loss: 0.2971 - accuracy: 0.8835 - val_loss: 0.1724 - val_accuracy: 0.9600\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9160\n",
            "Epoch 00015: val_loss did not improve from 0.17242\n",
            "50/50 [==============================] - 37s 731ms/step - loss: 0.2498 - accuracy: 0.9160 - val_loss: 0.4552 - val_accuracy: 0.8400\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9320\n",
            "Epoch 00016: val_loss improved from 0.17242 to 0.07709, saving model to /tmp/base.model\n",
            "INFO:tensorflow:Assets written to: /tmp/base.model/assets\n",
            "50/50 [==============================] - 33s 653ms/step - loss: 0.2496 - accuracy: 0.9320 - val_loss: 0.0771 - val_accuracy: 0.9600\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.9280\n",
            "Epoch 00017: val_loss did not improve from 0.07709\n",
            "50/50 [==============================] - 32s 631ms/step - loss: 0.2741 - accuracy: 0.9280 - val_loss: 0.9523 - val_accuracy: 0.8000\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9440\n",
            "Epoch 00018: val_loss did not improve from 0.07709\n",
            "50/50 [==============================] - 32s 632ms/step - loss: 0.1759 - accuracy: 0.9440 - val_loss: 0.2043 - val_accuracy: 0.9600\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.9280\n",
            "Epoch 00019: val_loss improved from 0.07709 to 0.04897, saving model to /tmp/base.model\n",
            "INFO:tensorflow:Assets written to: /tmp/base.model/assets\n",
            "50/50 [==============================] - 33s 651ms/step - loss: 0.2768 - accuracy: 0.9280 - val_loss: 0.0490 - val_accuracy: 0.9600\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.8960\n",
            "Epoch 00020: val_loss did not improve from 0.04897\n",
            "50/50 [==============================] - 32s 635ms/step - loss: 0.2833 - accuracy: 0.8960 - val_loss: 0.2115 - val_accuracy: 0.9600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb4350aa208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyepYOw9lSRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "799a61d6-e257-4179-be3a-9281d085bb40"
      },
      "source": [
        "%matplotlib inline\n",
        "upload = files.upload()\n",
        "#Test Model\n",
        "for fn in upload.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(220,220))\n",
        "  imgplot = plt.imshow(img)\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  hasil = np.argmax(classes)\n",
        "  print(fn)\n",
        "  if hasil==0:\n",
        "    print('paper')\n",
        "  elif hasil==1:\n",
        "    print('rock')\n",
        "  elif hasil==2:\n",
        "    print('scissors')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f66c1c03b959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mupload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2KJlVZj-TVY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}